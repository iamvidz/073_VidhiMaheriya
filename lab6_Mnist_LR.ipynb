{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab6_Mnist_LR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdMmAElROHic"
      },
      "source": [
        "#Importing libraries\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import io\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azI24WCaZWvc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOI65B-xOkJd"
      },
      "source": [
        "#MNIST -collection of hand-written digits- 60000 example for training, 10000 for testing\n",
        "#convert img to float32\n",
        "#normalize to [0,1]\n",
        "#flatten to a 1D array of 784 features (28x28)\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgGh1BOjZXc1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEPb9RVPRACI",
        "outputId": "df8ccb43-179e-4004-8b8a-cb751993a072"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Convert to float32.\n",
        "x_train = np.array(x_train, np.float32)\n",
        "x_test = np.array(x_test, np.float32)\n",
        "\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28).\n",
        "num_features=784\n",
        "x_train = x_train.reshape(60000, num_features)\n",
        "x_test = x_test.reshape(10000, num_features)\n",
        "\n",
        "\n",
        "# Normalize images value from [0, 255] to [0, 1].\n",
        "x_train = x_train / 255\n",
        "x_test = x_test /255\n",
        "\n",
        "x_train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_gpntLeZYXu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP_40rqFRAvq",
        "outputId": "b792052a-24dc-483c-dc5d-8004f775ddc9"
      },
      "source": [
        "x_test"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfXGGVoIZaBd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8No_BmkjREsU",
        "outputId": "14139f3d-b94f-4d45-e1e5-2a9fc7a3c2d8"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJkif9-oZalx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npABlkqwRHrW",
        "outputId": "d855872a-4687-4d76-9e32-3114c0ac0af3"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofGe_aDlRJmN",
        "outputId": "5d9703fd-9148-4f6b-e654-b04493df8375"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2EiG1tFRLYT"
      },
      "source": [
        "#setting up hyperparameters and data set parameters\n",
        "\n",
        "#initialize model parameters\n",
        "#num_class = number of outputs (10) (0 to 9 digits)\n",
        "#num_features = number of input para (784)\n",
        "\n",
        "# MNIST dataset parameters.\n",
        "\n",
        "num_classes = 10 # 0 to 9 digits\n",
        "\n",
        "num_features = 784 # 28*28\n",
        "\n",
        "# Training parameters.\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "display_step = 50"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLl1uoxfSEen"
      },
      "source": [
        "# shuffling and batching the data\n",
        "# before we start the actual training to avoid the model from getting biased by the data. \n",
        "# This will allow our data to be more random and helps our model to gain higher accuracies with the test data.\n",
        "# With the help of tf.data.Dataset.from_tensor_slices, we can get the slices of an array in the form of objects. \n",
        "# shuffle(5000) randomizes the order of the data setâ€™s examples. \n",
        "# Here, 5000 denotes the variable shuffle_buffer, which tells the model to pick a sample randomly from 1 to 5000 samples. \n",
        "# After that, only 4999 samples are left in the buffer, so the sample 5001 gets added to the buffer."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6O-c1ybaQnM"
      },
      "source": [
        "# Use tf.data API to shuffle and batch data.\n",
        "num_batches = int(x_train.shape[0] / batch_size)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoBuMB4eaTG6"
      },
      "source": [
        "#initializing weights and biases\n",
        "#with ones and zeros\n",
        "\n",
        "# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\n",
        "\n",
        "W = tf.Variable(np.random.randn(784, 10).astype(np.float32))\n",
        "\n",
        "# Bias of shape [10], the total number of classes.\n",
        "B = tf.Variable(np.random.randn(10).astype(np.float32))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiI5byChagkM"
      },
      "source": [
        "#defining logistic regression and cost function\n",
        "\n",
        "#which converts the inputs into a probability distribution proportional to the exponents of the inputs using the softmax function. \n",
        "#The softmax function, which is implemented using the function tf.nn.softmax, also makes sure that the sum of all the inputs equals one.\n",
        "\n",
        "# Logistic regression (Wx + b).\n",
        "\n",
        "def logistic_regression(x):\n",
        "\n",
        "    # Apply softmax to normalize the logits to a probability distribution.\n",
        "    return tf.nn.softmax(tf.add(tf.matmul(x, W), B))\n",
        "    \n",
        "\n",
        "# Cross-Entropy loss function.\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "\n",
        "    # Encode label to a one hot vector.\n",
        "    y_true = tf.one_hot(y_true, depth = num_classes)\n",
        "    \n",
        "\n",
        "    # Clip prediction values to avoid log(0) error.\n",
        "\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)    \n",
        "\n",
        "    # Compute cross-entropy.\n",
        "    loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
        "    return loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMwkxh2ca0lb"
      },
      "source": [
        "# Defining Optimizers and Accuracy Metrics\n",
        "\n",
        "# When we compute the output, it gives us the probability of the given data to fit a particular class of output. \n",
        "# We consider the correct prediction as to the class having the highest probability. \n",
        "# We compute this using the function tf.argmax. \n",
        "# We also define the stochastic gradient descent as the optimizer from several optimizers present in TensorFlow. We do this using the function tf.optimizers.SGD. \n",
        "# This function takes in the learning rate as its input, which defines how fast the model should reach its minimum loss or gain the highest accuracy."
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCAKntZ2bO6Q"
      },
      "source": [
        "# Accuracy metric.\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "\n",
        "  # Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
        "\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6DHVMjbhQI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_1loRNzbmMe"
      },
      "source": [
        "Optimization Process and Updating Weights and Biases\n",
        "\n",
        "Now we define the run_optimization() method where we update the weights of our model. We calculate the predictions using the logistic_regression(x) method by taking the inputs and find out the loss generated by comparing the predicted value and the original value present in the data set. Next, we compute the gradients using and update the weights of the model with our stochastic gradient descent optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZecM6ZSmbsDO"
      },
      "source": [
        "# Optimization process. \n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "def run_optimization(x, y):\n",
        "\n",
        "# Wrap computation inside a GradientTape for automatic differentiation.\n",
        "\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = logistic_regression(x)\n",
        "        loss = cross_entropy(pred, y)\n",
        "\n",
        "    # Compute gradients.\n",
        "\n",
        "    gradients = g.gradient(loss, [W, B])\n",
        "    # Stochastic gradient descent optimizer.\n",
        "    # Update W and b following gradients.\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, [W, B]))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS4PYC3YbwLk",
        "outputId": "5f770063-2046-419d-d46f-777fc9a743a3"
      },
      "source": [
        "# Run training for the given number of steps.\n",
        "\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "\n",
        "    # Run the optimization to update W and b values.\n",
        "\n",
        "    run_optimization(batch_x, batch_y)\n",
        "    if step % display_step == 0:\n",
        "\n",
        "        #Obtain Predictions\n",
        "        pred = logistic_regression(batch_x)\n",
        "        #Ccompute loss\n",
        "        loss = cross_entropy(pred, batch_y)\n",
        "        #Compute Accuracy\n",
        "        acc = accuracy(pred, batch_y)\n",
        "        #print accuracy\n",
        "        print(f\"step: {step}, loss: {loss}, accuracy: {acc}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 50, loss: 320.8542785644531, accuracy: 0.7890625\n",
            "step: 100, loss: 302.1634826660156, accuracy: 0.79296875\n",
            "step: 150, loss: 168.8983917236328, accuracy: 0.85546875\n",
            "step: 200, loss: 152.08096313476562, accuracy: 0.83984375\n",
            "step: 250, loss: 82.09939575195312, accuracy: 0.90625\n",
            "step: 300, loss: 104.72555541992188, accuracy: 0.88671875\n",
            "step: 350, loss: 131.3424530029297, accuracy: 0.90625\n",
            "step: 400, loss: 73.15220642089844, accuracy: 0.91796875\n",
            "step: 450, loss: 65.37161254882812, accuracy: 0.92578125\n",
            "step: 500, loss: 107.53083801269531, accuracy: 0.8828125\n",
            "step: 550, loss: 99.3916015625, accuracy: 0.8671875\n",
            "step: 600, loss: 157.05404663085938, accuracy: 0.8359375\n",
            "step: 650, loss: 86.279296875, accuracy: 0.89453125\n",
            "step: 700, loss: 70.87052154541016, accuracy: 0.91796875\n",
            "step: 750, loss: 77.57180786132812, accuracy: 0.94140625\n",
            "step: 800, loss: 74.92724609375, accuracy: 0.921875\n",
            "step: 850, loss: 43.148292541503906, accuracy: 0.94921875\n",
            "step: 900, loss: 83.86661529541016, accuracy: 0.9140625\n",
            "step: 950, loss: 91.99604797363281, accuracy: 0.92578125\n",
            "step: 1000, loss: 74.76268005371094, accuracy: 0.90625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "przJJPwWbzOU",
        "outputId": "491dd1a6-0d76-485b-a34c-f18ee0922d21"
      },
      "source": [
        "# Test model on validation set.\n",
        "pred = logistic_regression(x_test)\n",
        "a = accuracy(pred, y_test)\n",
        "print(f\"Test Accuracy: {a}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8981000185012817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xZVn2iib3fs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGRtxonhbpXc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}